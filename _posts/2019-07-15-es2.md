---
title: Elasticsearch-2
date: 2019-07-15 13:54:00 +0800
categories: [数据库]
tags: [Elasticsearch,Web开发技术]
---
# 索引与类型

## 索引

查看索引

```shell
curl 127.0.0.1:9200/_cat/indices
```

> 请求`curl 127.0.0.1:9200/_cat`可获取用于查询的名称

创建索引

索引可以在添加文档数据时，通过动态映射的方式自动生成索引与类型。

索引也可以手动创建，通过手动创建，可以控制主分片数目、分析器和类型映射。

```http
PUT /my_index
{
    "settings": { ... any settings ... },
    "mappings": {
        "type_one": { ... any mappings ... },
        "type_two": { ... any mappings ... },
        ...
    }
}
```

**注： 在Elasticsearch 5.x版本中，设置分片与设置索引的类型字段需要分两次设置完成。**

删除索引

用以下的请求来 删除索引:

```http
DELETE /my_index
```

你也可以这样删除多个索引：

```http
DELETE /index_one,index_two
DELETE /index_*
```

你甚至可以这样删除 *全部* 索引：

```http
DELETE /_all
DELETE /*
```

## 类型和映射

*类型* 在 Elasticsearch 中表示一类相似的文档，类型由 *名称* 和 *映射* （ mapping）组成。

*映射*, mapping， 就像数据库中的 schema ，描述了文档可能具有的字段或 *属性* 、 每个字段的数据类型—比如 `string`, `integer` 或 `date`。

为了能够将时间字段视为时间，数字字段视为数字，字符串字段视为全文或精确值字符串， Elasticsearch 需要知道每个字段中数据的类型。

简单字段类型：

- 字符串: `text` (在elaticsearch 2.x版本中，为string类型)
- 整数 : `byte`, `short`, `integer`, `long`
- 浮点数: `float`, `double`
- 布尔型: `boolean`
- 日期: `date`

```shell
curl -X PUT 127.0.0.1:9200/example/_mapping/article -H 'Content-Type: application/json' -d'
{
     "_all": {
          "analyzer": "ik_max_word"
      },
      "properties": {
          "article_id": {
              "type": "long",
              "include_in_all": "false"
          },
          "user_id": {
              "type": "long",
              "include_in_all": "false"
          },
          "title": {
              "type": "text",
              "analyzer": "ik_max_word",
              "include_in_all": "true",
              "boost": 2
          },
          "content": {
              "type": "text",
              "analyzer": "ik_max_word",
              "include_in_all": "true"
          },
          "status": {
              "type": "integer",
              "include_in_all": "false"
          },
          "create_time": {
              "type": "date",
              "include_in_all": "false"
          }
      }
}
'
```

- `_all`字段是把所有其它字段中的值，以空格为分隔符组成一个大字符串，然后被分析和索引，但是不存储，也就是说它能被查询，但不能被取回显示。`_all`允许在不知道要查找的内容是属于哪个具体字段的情况下进行搜索。

- `analyzer`指明使用的分析器

    索引时的顺序如下：

    - 字段映射里定义的 `analyzer`

    - 否则索引设置中名为 `default` 的分析器，默认为`standard` 标准分析器

        在搜索时，顺序有些许不同：

    - 查询自己定义的 `analyzer`

    - 否则字段映射里定义的 `analyzer`

    - 否则索引设置中名为 `default` 的分析器，默认为`standard` 标准分析器

- `include_in_all` 参数用于控制 `_all` 查询时需要包含的字段。默认为 true。

- `boost`可以提升查询时计算相关性分数的权重。例如`title`字段将是其他字段权重的两倍。

一个类型映射创建好后，可以为类型增加新的字段映射

```shell
curl -X PUT 127.0.0.1:9200/articles/_mapping/article -H 'Content-Type:application/json' -d '
{
  "properties": {
    "new_tag": {
      "type": "text"
    }
  }
}
'
```

**但是不能修改已有字段的类型映射，原因在于elasticsearch已按照原有字段映射生成了反向索引数据，类型映射改变意味着需要重新构建反向索引数据，所以并不能再原有基础上修改，只能新建索引库，然后创建类型映射后重新构建反向索引数据。**

## 为索引起别名

为索引起别名，让新建的索引具有原索引的名字，可以让应用程序零停机。

```shell
curl -X DELETE 127.0.0.1:9200/articles
curl -X PUT 127.0.0.1:9200/articles_v2/_alias/articles
```

查询索引别名

```shell
# 查看别名指向哪个索引
curl 127.0.0.1:9200/*/_alias/articles

# 查看哪些别名指向这个索引
curl 127.0.0.1:9200/articles_v2/_alias/*
```

# 文档

一个文档的实例

```shell
{
    "name":         "John Smith",
    "age":          42,
    "confirmed":    true,
    "join_date":    "2014-06-01",
    "home": {
        "lat":      51.5,
        "lon":      0.1
    },
    "accounts": [
        {
            "type": "facebook",
            "id":   "johnsmith"
        },
        {
            "type": "twitter",
            "id":   "johnsmith"
        }
    ]
}
```

一个文档不仅仅包含它的数据 ，也包含 *元数据*(metadata) —— 有关文档的信息。 三个必须的元数据元素如下：

- `_index`

    文档在哪存放

- `_type`

    文档表示的对象类别

- `_id`

    文档唯一标识

## 索引文档（保存文档数据）

使用自定义的文档id

```shell
PUT /{index}/{type}/{id}
{
  "field": "value",
  ...
}
```

```shell
curl -X PUT 127.0.0.1:9200/articles/article/150000 -H 'Content-Type:application/json' -d '
{
  "article_id": 150000,
  "user_id": 1,
  "title": "python",
  "content": "确实如此",
  "status": 2,
  "create_time": "2019-04-03"
}'
```

## 获取指定文档

```shell
curl 127.0.0.1:9200/articles/article/150000?pretty

# 获取一部分
curl 127.0.0.1:9200/articles/article/150000?_source=title,content\&pretty
```

注意：`_version` 每次修改文档数据，版本都会增加，可以当作乐观锁的依赖（判断标准）使用

## 判断文档是否存在

```shell
curl -i -X HEAD 127.0.0.1:9200/articles/article/150000
```

- 存在 200状态码
- 不存在 404状态码

## 更新文档

在 Elasticsearch 中文档是 *不可改变* 的，不能修改它们。 相反，如果想要更新现有的文档，需要 *重建索引*或者进行替换。我们可以使用相同的 `index` API 进行实现。

例如修改title字段的内容，不可进行以下操作（仅传递title字段内容）

```shell
curl -X PUT 127.0.0.1:9200/articles/article/150000 -H 'Content-Type:application/json' -d '
{
  "title": "python必须是世界上最好的语言"
}'
```

而是要索引完整文档内容

```shell
curl -X PUT 127.0.0.1:9200/articles/article/150000 -H 'Content-Type:application/json' -d '
{
  "article_id": 150000,
  "user_id": 1,
  "title": "python必须是世界上最好的语言",
  "content": "确实如此",
  "status": 2,
  "create_time": "2019-04-03"
}'
```

## 删除文档

```shell
curl -X DELETE 127.0.0.1:9200/articles/article/150000
```

# Logstash导入数据

```shell
# 安装
sudo rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
```

在 /etc/yum.repos.d/ 中创建logstash.repo文件

```
[logstash-6.x]
name=Elastic repository for 6.x packages
baseurl=https://artifacts.elastic.co/packages/6.x/yum
gpgcheck=1
gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
enabled=1
autorefresh=1
type=rpm-md
```

执行

```shell
sudo yum install logstash
cd /usr/share/logstash/bin/
sudo ./logstash-plugin install logstash-input-jdbc
sudo ./logstash-plugin install logstash-output-elasticsearch
scp mysql-connector-java-8.0.13.tar.gz python@10.211.55.7:~/
tar -zxvf mysql-connector-java-8.0.13.tar.gz
```

**从MySQL导入数据到Elasticsearch**

创建配置文件logstash_mysql.conf

```shell
# 读取数据的来源
input{
     jdbc {
         # java读取mysql数据的驱动程序 
         jdbc_driver_library => "/home/zhao/mysql-connector-java-8.0.13/mysql-connector-java-8.0.13.jar"
         
         jdbc_driver_class => "com.mysql.jdbc.Driver"
         
         # mysql数据库的配置信息
         jdbc_connection_string => "jdbc:mysql://127.0.0.1:3306/test?tinyInt1isBit=false"
         jdbc_user => "root"
         jdbc_password => "mysql"
         
         # 是否分页分批次从mysql中读取数据再写入es
         jdbc_paging_enabled => "true"
         jdbc_page_size => "1000"
         
         
         jdbc_default_timezone =>"Asia/Shanghai"
         
         # 执行读取mysql数据库操作的sql语句
         # mysql查询的结果字段一定要与es中表字段名对应相同
         statement => "select a.article_id as article_id,a.user_id as user_id, a.title as title, a.status as status, a.create_time as create_time,  b.content as content from news_article_basic as a inner join news_article_content as b on a.article_id=b.article_id"
         
         # 我们想要让es中保存的每个文档数据的唯一编号_id 与mysql中的文章表主键article_id 一直，所以需要logstash 追踪mysql中每条记录的article_id字段的值
         use_column_value => "true"
         tracking_column => "article_id"
         
         # 是否记录上次执行结果, 如果为真,将会把上次执行到的 tracking_column 字段的值记录下来,保存到 last_run_metadata_path 指定的文件中
         record_last_run => true
		 last_run_metadata_path => "./config/station_parameter.txt"
         
         # 是否清除last_run_metadata_path的记录,如果为真那么每次相当于从头开始查询所有的数据库记录
         clean_run => true
     }
}

# 数据写入保存的地方
output{
      elasticsearch {
         hosts => "127.0.0.1:9200"
         index => "articles"
         document_id => "%{article_id}"
         document_type => "article"
      }
      stdout {
         codec => json_lines
     }
}
```

```shell
sudo /usr/share/logstash/bin/logstash -f ./logstash_mysql.conf
```

